{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Least Squares Support Vector Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "\n",
    "2. [Using the classifier](#using_classifier)\n",
    "\n",
    "    2.1 [CPU/Numpy version](#cpu_version)\n",
    "    \n",
    "    2.2 [GPU/PyTorch version](#gpu_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least Squares Support Vector Machine (LSSVM) is a variation of the original Support Vector Machine (SVM) in which we have a slight change in the objective and restriction functions that results in a big simplification of the optimization problem.\n",
    "\n",
    "First, let's see the optimization problem of an SVM:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + C \\sum_{i=1}^{n} \\xi_i &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b)\\geq 1 - \\xi_i, && i = 1,..., n \\\\\n",
    "         && \\xi_i \\geq 0,                            && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, we have a set of inequality restrictions and when solving the optimization problem by it's dual we find a discriminative function, adding the kernel trick, of the type:\n",
    "\n",
    "\n",
    "$$ f(\\vec{x}) = sign \\ \\Big( \\sum_{i=1}^{n} \\alpha_i^o y_i K(\\vec{x}_i,\\vec{x}) + b_o \\Big) $$\n",
    "\n",
    "Where $\\alpha_i^o$ and $b_o$ denote optimum values. Giving enough regularization (smaller values of $C$) we get a lot of $\\alpha_i^o$ nulls, resulting in a sparse model in which we only need to save the pairs $(\\vec{x}_i,y_i)$ which have the optimum dual variable not null. The vectors $\\vec{x}_i$ with not null $\\alpha_i^o$ are known as support vectors (SV).\n",
    "\n",
    "\n",
    "\n",
    "In the LSSVM case, we change the inequality restrictions to equality restrictions. As the $\\xi_i$ may be negative we square its values in the objective function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + \\gamma \\frac{1}{2}\\sum_{i=1}^{n} \\xi_i^2 &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b) = 1 - \\xi_i, && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The dual of this optimization problem results in a system of linear equations, a set of Karush-Khun-Tucker (KKT) equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    0 & \\vec{d}^T \\\\\n",
    "    \\vec{y} & \\Omega + \\gamma^{-1} I \n",
    "\\end{bmatrix}\n",
    "\\\n",
    "\\begin{bmatrix} \n",
    "    b  \\\\\n",
    "    \\vec{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    0 \\\\\n",
    "    \\vec{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where, with the kernel trick, &nbsp; $\\Omega_{i,j} = y_i y_j K(\\vec{x}_i,\\vec{x}_j)$,  &nbsp;  $\\vec{y} = [y_1 \\ y_2 \\ ... \\ y_n]^T$, &nbsp; $\\vec{\\alpha} = [\\alpha_1 \\ \\alpha_2 \\ ... \\ \\alpha_n]^T$ &nbsp;  e &nbsp; $\\vec{1} = [1 \\ 1 \\ ... \\ 1]^T$.\n",
    "\n",
    "The discriminative function of the LSSVM has the same form of the SVM but the $\\alpha_i^o$ aren't usually null, resulting in a bigger model. The big advantage of the LSSVM is in finding it's parameters, which is reduced to solving the linear system of the type:\n",
    "\n",
    "$$ A\\vec{z} = \\vec{b} $$\n",
    "\n",
    "A well-known solution of the linear system is when we minimize the square of the residues, that can be written as the optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{z})=\\frac{1}{2}||A\\vec{z} - \\vec{b}||^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And have the analytical solution:\n",
    "\n",
    "$$ \\vec{z} = A^{\\dagger} \\vec{b} $$\n",
    "\n",
    "Where $A^{\\dagger}$ is the pseudo-inverse defined as:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using the classifier <a class=\"anchor\" id=\"using_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lssvm import LSSVC, LSSVC_GPU\n",
    "from utils.encoding import dummie2multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1078, 64)\n",
      "X_test.shape:  (719, 64)\n",
      "y_train.shape: (1078,)\n",
      "y_test.shape:  (719,)\n",
      "np.unique(y_train): [0 1 2 3 4 5 6 7 8 9]\n",
      "np.unique(y_test):  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Import digits recognition dataset (from sklearn)\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=2020\n",
    ")\n",
    "\n",
    "# Scaling features (from sklearn)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_tr_norm = scaler.transform(X_train)\n",
    "X_ts_norm = scaler.transform(X_test)\n",
    "\n",
    "# Get information about input and outputs\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape:  {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape:  {y_test.shape}\")\n",
    "print(f\"np.unique(y_train): {np.unique(y_train)}\")\n",
    "print(f\"np.unique(y_test):  {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CPU/Numpy version <a class=\"anchor\" id=\"cpu_version\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "acc_test =  0.9680111265646731 \n",
      "\n",
      "Polynomial kernel:\n",
      "acc_test =  0.9944367176634215 \n",
      "\n",
      "Linear kernel:\n",
      "acc_test =  0.9791376912378303 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the classifier with different kernels\n",
    "\n",
    "print(\"Gaussian kernel:\")\n",
    "lssvc = LSSVC(gamma=1, kernel=\"rbf\", sigma=0.5)  # Class instantiation\n",
    "lssvc.fit(X_tr_norm, y_train)  # Fitting the model\n",
    "y_pred = lssvc.predict(X_ts_norm)  # Making predictions with the trained model\n",
    "acc = accuracy_score(\n",
    "    dummie2multilabel(y_test), dummie2multilabel(y_pred)\n",
    ")  # Calculate Accuracy\n",
    "print(\"acc_test = \", acc, \"\\n\")\n",
    "\n",
    "print(\"Polynomial kernel:\")\n",
    "lssvc = LSSVC(gamma=1, kernel=\"poly\", d=2)\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print(\"acc_test = \", acc, \"\\n\")\n",
    "\n",
    "print(\"Linear kernel:\")\n",
    "lssvc = LSSVC(gamma=1, kernel=\"linear\")\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print(\"acc_test = \", acc, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any questions about a specific method, the user can ask for help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module lssvm.LSSVC:\n",
      "\n",
      "fit(self, X, y)\n",
      "    Fits the model given the set of X attribute vectors and y labels.\n",
      "    - X: ndarray of shape (n_samples, n_attributes)\n",
      "    - y: ndarray of shape (n_samples,) or (n_samples, n)\n",
      "        If the label is represented by an array of n elements, the y \n",
      "        parameter must have n columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LSSVC.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also have an overview of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSSVC in module lssvm.LSSVC:\n",
      "\n",
      "class LSSVC(builtins.object)\n",
      " |  LSSVC(gamma=1, kernel='rbf', **kernel_params)\n",
      " |  \n",
      " |  A class that implements the Least Squares Support Vector Machine \n",
      " |  for classification tasks.\n",
      " |  \n",
      " |  It uses Numpy pseudo-inverse function to solve the dual optimization \n",
      " |  problem with ordinary least squares. In multiclass classification \n",
      " |  problems the approach used is one-vs-all, so, a model is fit for each \n",
      " |  class while considering the others a single set of the same class.\n",
      " |  \n",
      " |  # Parameters:\n",
      " |  - gamma: float, default = 1.0\n",
      " |      Constant that control the regularization of the model, it may vary \n",
      " |      in the set (0, +infinity). The closer gamma is to zero, the more \n",
      " |      regularized the model will be.\n",
      " |  - kernel: {'linear', 'poly', 'rbf'}, default = 'rbf'\n",
      " |      The kernel used for the model, if set to 'linear' the model \n",
      " |      will not take advantage of the kernel trick, and the LSSVC maybe only\n",
      " |      useful for linearly separable problems.\n",
      " |  - kernel_params: **kwargs, default = depends on 'kernel' choice\n",
      " |      If kernel = 'linear', these parameters are ignored. If kernel = 'poly',\n",
      " |      'd' is accepted to set the degree of the polynomial, with default = 3. \n",
      " |      If kernel = 'rbf', 'sigma' is accepted to set the radius of the \n",
      " |      gaussian function, with default = 1. \n",
      " |   \n",
      " |  # Attributes:\n",
      " |  - All hyperparameters of section \"Parameters\".\n",
      " |  - alpha: ndarray of shape (1, n_support_vectors) if in binary \n",
      " |           classification and (n_classes, n_support_vectors) for \n",
      " |           multiclass problems\n",
      " |      Each column is the optimum value of the dual variable for each model\n",
      " |      (using the one-vs-all approach we have n_classes == n_classifiers), \n",
      " |      it can be seen as the weight given to the support vectors \n",
      " |      (sv_x, sv_y). As usually there is no alpha == 0, we have \n",
      " |      n_support_vectors == n_train_samples.\n",
      " |  - b: ndarray of shape (1,) if in binary classification and (n_classes,) \n",
      " |       for multiclass problems \n",
      " |      The optimum value of the bias of the model.\n",
      " |  - sv_x: ndarray of shape (n_support_vectors, n_features)\n",
      " |      The set of the supporting vectors attributes, it has the shape \n",
      " |      of the training data.\n",
      " |  - sv_y: ndarray of shape (n_support_vectors, n)\n",
      " |      The set of the supporting vectors labels. If the label is represented \n",
      " |      by an array of n elements, the sv_y attribute will have n columns.\n",
      " |  - y_labels: ndarray of shape (n_classes, n)\n",
      " |      The set of unique labels. If the label is represented by an array \n",
      " |      of n elements, the y_label attribute will have n columns.\n",
      " |  - K: function, default = rbf()\n",
      " |      Kernel function.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, gamma=1, kernel='rbf', **kernel_params)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  dump(self, filepath='model', only_hyperparams=False)\n",
      " |      This method saves the model in a JSON format.\n",
      " |      - filepath: string, default = 'model'\n",
      " |          File path to save the model's json.\n",
      " |      - only_hyperparams: boolean, default = False\n",
      " |          To either save only the model's hyperparameters or not, it \n",
      " |          only affects trained/fitted models.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fits the model given the set of X attribute vectors and y labels.\n",
      " |      - X: ndarray of shape (n_samples, n_attributes)\n",
      " |      - y: ndarray of shape (n_samples,) or (n_samples, n)\n",
      " |          If the label is represented by an array of n elements, the y \n",
      " |          parameter must have n columns.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predicts the labels of data X given a trained model.\n",
      " |      - X: ndarray of shape (n_samples, n_attributes)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(filepath, only_hyperparams=False) from builtins.type\n",
      " |      This class method loads a model from a .json file.\n",
      " |      - filepath: string\n",
      " |          The model's .json file path.\n",
      " |      - only_hyperparams: boolean, default = False\n",
      " |          To either load only the model's hyperparameters or not, it \n",
      " |          only has effects when the dump of the model as done with the\n",
      " |          model's parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LSSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also save the model in JSON format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.9791376912378303\n",
      "acc_test =  0.9791376912378303\n"
     ]
    }
   ],
   "source": [
    "lssvc.dump(\"model\")\n",
    "loaded_model = LSSVC.load(\"model\")\n",
    "\n",
    "# Showing the same results\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(loaded_model.predict(X_ts_norm))\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPU/PyTorch version <a class=\"anchor\" id=\"gpu_version\"></a>    \n",
    "\n",
    "It has the same functionalities and syntax of the CPU version, the difference is the use of PyTorch to run the operations on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "acc_test =  0.9680111265646731 \n",
      "\n",
      "Polynomial kernel:\n",
      "acc_test =  0.9944367176634215 \n",
      "\n",
      "Linear kernel:\n",
      "acc_test =  0.9791376912378303 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the classifier with different kernels\n",
    "\n",
    "print(\"Gaussian kernel:\")\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel=\"rbf\", sigma=0.5)\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)),\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "print(\"Polynomial kernel:\")\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel=\"poly\", d=2)\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)),\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "print(\"Linear kernel:\")\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel=\"linear\")\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)),\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also save the model in JSON format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.9791376912378303\n",
      "acc_test =  0.9791376912378303\n"
     ]
    }
   ],
   "source": [
    "lssvc_gpu.dump(\"model\")\n",
    "loaded_model = LSSVC_GPU.load(\"model\")\n",
    "\n",
    "# Showing the same results\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test),\n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm).cpu()),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.json` is the same for the CPU and GPU version, giving the developer the possibility to train a model in GPU, dumping it in a .json, and loading in CPU version (the other way around is also possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.9791376912378303\n",
      "acc_test =  0.9791376912378303\n"
     ]
    }
   ],
   "source": [
    "lssvc.dump(\"model_from_cpu\")\n",
    "lssvc_gpu = LSSVC_GPU.load(\"model_from_cpu\")\n",
    "\n",
    "# Showing the same results\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.9791376912378303\n",
      "acc_test =  0.9791376912378303\n"
     ]
    }
   ],
   "source": [
    "lssvc_gpu.dump(\"model_from_gpu\")\n",
    "lssvc = LSSVC.load(\"model_from_gpu\")\n",
    "\n",
    "# Showing the same results\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"acc_test = \",\n",
    "    accuracy_score(\n",
    "        dummie2multilabel(y_test), dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
